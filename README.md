# FedSAM on CIFAR-10 and MNIST with MobileNet (PyTorch)

This repository provides a PyTorch implementation of FedSAM (Generalized Federated Learning via Sharpness-Aware Minimization) using MobileNet on CIFAR-10 and MNIST. 

FedSAM improves robustness under Non-IID data by training each client with a sharpness-aware objective (SAM), then aggregating client models on the server.

â¸»

## Project Overview

This simulation covers:
	â€¢	Data Partitioning: IID and Non-IID splits (Dirichlet distribution).
	â€¢	Client-side FedSAM: local training uses two-step SAM update (perturb â†’ descent).
	â€¢	Server-side Aggregation: standard weighted FedAvg aggregation of client models (Same as FedAVG aggregation).
	â€¢	BatchNorm Handling (stability):
	â€¢	SAM training typically disables BN running-stat updates during the first forward pass.
	â€¢	Server aggregation can average BN buffers (running_mean/var) with standard averaging (no special correction).

â¸»

## Recommended Folder Structure

Your `main.py` imports modules like `data.cifar10`, `models.mobilenet,` etc.
Organize files like this to run without modifying imports:
```
â”œâ”€â”€ main.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cifar10.py
â”‚   â”œâ”€â”€ mnist.py
â”‚   â””â”€â”€ partition.py
â”œâ”€â”€ fl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fedsam.py
â”‚   â””â”€â”€ server.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mobilenet.py
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ device.py
    â”œâ”€â”€ eval.py
    â”œâ”€â”€ parser.py
    â””â”€â”€ seed.py
```

## Requirements

- Python 3.9+ recommended
- PyTorch + torchvision
- numpy

Run with default settings:
```bash
python main.py
```
Example1: Non-IID
```bash
python main.py --partition niid
```
Example2: Non-IID with control dyn-alpha
```bash
python main.py --partition niid --alpha 0.5 --min-size 10
```
Example3: Change the number and ratio of participating clients + local epoch
```bash
python main.py --num-clients 100 --client-frac 0.2 --local-epochs 5
```

## Device Selection

The code supports:
```
	â€¢	--device auto (default): selects CUDA if available, else MPS (Apple Silicon), else CPU
	â€¢	--device cuda
	â€¢	--device mps
	â€¢	--device cpu
```

Example:
```bash
python main.py --device auto
```

## CLI Arguments

Key arguments (from utils/parser.py):
```
	â€¢	Reproducibility / compute
	  â€¢	--seed (default: 845)
	  â€¢	--device in {auto,cpu,cuda,mps}

	â€¢	Training method
	  â€¢	--sam-rho (FedSAM rho, default 0.05)

	â€¢	Dataset
	  â€¢   --data-set (default cifar10, choices=[cifar10, mnist])
	  â€¢	--data-root (default ./data)
	  â€¢	--augment / --no-augment
	  â€¢	--normalize / --no-normalize
	  â€¢	--test-batch-size (default 128)

	â€¢	Federated learning config
	  â€¢	--num-clients (default 10)
	  â€¢	--client-frac fraction of clients sampled per round (default 0.25)
	  â€¢	--local-epochs (default 1)
	  â€¢	--batch-size (default 100)
	  â€¢	--lr learning rate (default 1e-2)
	  â€¢	--rounds communication rounds (default 10)

	â€¢	Data partitioning
	  â€¢	--partition in {iid,niid}
	  â€¢	--alpha: Dirichlet concentration parameter controlling Non-IID severity.
		      â”œâ”€â”€ Î± = 0.1 ~ 0.3: highly skewed label distribution (strong Non-IID)
		  	  â”œâ”€â”€	Î± = 0.5: moderate Non-IID (default)
		  	  â””â”€â”€	Î± = 0.8 ~ 1.0: closer to IID
	  â€¢	--min-size minimum samples per client in non-IID (default 10)
	  â€¢	--print-labels / --no-print-labels

	â€¢	Learning rate Scheduler (ReduceOnPlateau)
	  â€¢	--lr-factor (learning rate * factor, default 0.5)
	  â€¢	--lr-patience (default 5)
	  â€¢	--min-lr (deafult 1e-6)
	  â€¢	--lr-threshold (default 1e-4)
	  â€¢	--lr-cooldown (default 0)
```

## FedSAM Implementation Notes

### 1) Client-side FedSAM Update (fl/fedsam.py)

FedSAM applies Sharpness-Aware Minimization (SAM) locally on each client.

**SAM objective (per client):**
$$\min_{w}\; \max_{\|\epsilon\|\le\rho}\; \mathcal{L}_k(w + \epsilon)$$

**A standard two-step SAM update is:**


1.	**Compute gradient at current weights:**

    $$\nabla g = \nabla_ \mathcal{L}_k(w)$$

2.	**Perturb weights toward the gradient direction:**
	
    $$\epsilon^{*} = \rho \frac{\nabla_{\theta} \mathcal{L}_k(\theta)}{\|\nabla_{\theta} \mathcal{L}_k(\theta)\|_2}$$

3.	**Compute gradient at perturbed weights and do the descent step:**
	
	$$\quad w \leftarrow w - \eta \nabla \mathcal{L}(w + \epsilon^*)$$

BatchNorm note (recommended): During the â€œperturb forward/backwardâ€, many SAM implementations disable BN running-stat updates for stability.

Optimizer: typically SGD with weight decay.

â¸»

## 2) Server-side Aggregation (fl/server.py)

**The server aggregates client models with standard weighted averaging (FedAvg)**:

$$w_{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} w_k^t$$

* (**n_k**: number of samples at client k)

* **BatchNorm buffers (running_mean/var, num_batches_tracked):** In practice, you can aggregate them using the same weighted averaging (or optionally keep them local depending on your setting).

## Expected Output

Each round prints evaluation results like:
```bash
=== Evaluate global model 1 Round ===
[01] acc=XX.XX%, loss=Y.YYYYYY
```

With data_set="cifar10", num_clients=100, client_frac=0.25, local_epochs=5, batch_size=50, lr=1e-2, rounds=200, partition="niid", alpha=0.4, lr_patience=10, min_lr=1e-5:
<br>79 Round ACC=60.91%, loss=1.128906
<br>85 Round ACC=62.36%, loss=1.098357
<br>93 Round ACC=65.28%, loss=1.024822
<br>109 Round ACC=69.61%, loss=0.944319
<br>132 Round ACC=73.68%, loss=0.800382
<br>151 Round ACC=76.91%, loss=0.739485
<br>166 Round ACC=78.16%, loss=0.676211
<br>183 Round ACC=78.83%, loss=0.631958
<br>191 Round ACC=81.23%, loss=0.580031
<br>200 Round ACC=81.77%, loss=0.594970
## ğŸ› ï¸ FedSAM Implementation Notes

### 1) Client-side FedSAM Update (`fl/fedsam.py`)

FedSAMì€ ê° í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë¡œì»¬ë¡œ Sharpness-Aware Minimization(SAM)ì„ ì ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì…ë‹ˆë‹¤.

**SAM ëª©ì  í•¨ìˆ˜ (í´ë¼ì´ì–¸íŠ¸ë³„):**

$$\min_{\theta}\; \max_{\|\epsilon\|\le\rho}\; \mathcal{L}_k(\theta + \epsilon)$$

**í‘œì¤€ 2ë‹¨ê³„ SAM ì—…ë°ì´íŠ¸ ê³¼ì •:**

1. **í˜„ì¬ ê°€ì¤‘ì¹˜ì—ì„œ ê·¸ë ˆì´ë””ì–¸íŠ¸ ê³„ì‚°:**

   $$\nabla g = \nabla_{\theta} \mathcal{L}_k(\theta)$$

2. **ê·¸ë ˆì´ë””ì–¸íŠ¸ ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì„­ë™(Perturbation) ì ìš©:**

   $$\epsilon^* = \rho \frac{\nabla_{\theta} \mathcal{L}_k(\theta)}{\|\nabla_{\theta} \mathcal{L}_k(\theta)\|_2}$$

3. **ì„­ë™ëœ ì§€ì ì—ì„œ ê·¸ë ˆì´ë””ì–¸íŠ¸ ê³„ì‚° ë° ìµœì¢… ì—…ë°ì´íŠ¸:**

   $$\theta \leftarrow \theta - \eta \nabla_{\theta} \mathcal{L}_k(\theta + \epsilon^*)$$

* **BatchNorm ì°¸ê³ **: ì•ˆì •ì„±ì„ ìœ„í•´ ì„­ë™ ë‹¨ê³„(Perturb forward/backward) ë™ì•ˆì—ëŠ” BatchNormì˜ running statistics ì—…ë°ì´íŠ¸ë¥¼ ë¹„í™œì„±í™”í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.
* **Optimizer**: ì¼ë°˜ì ìœ¼ë¡œ Weight Decayê°€ í¬í•¨ëœ SGDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

---

## ğŸ›ï¸ 2) Server-side Aggregation (`fl/server.py`)

**ì„œë²„ëŠ” í‘œì¤€ ê°€ì¤‘ í‰ê·  ë°©ì‹(FedAvg)ì„ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ ëª¨ë¸ì„ ë³‘í•©í•©ë‹ˆë‹¤:**

$$w_{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} w_k^t$$

* $n_k$: í´ë¼ì´ì–¸íŠ¸ $k$ê°€ ë³´ìœ í•œ ìƒ˜í”Œ ìˆ˜ì…ë‹ˆë‹¤.
* **BatchNorm ë²„í¼**: `running_mean`, `running_var` ë“±ì˜ ë²„í¼ëŠ” ê°€ì¤‘ í‰ê· ì„ í†µí•´ ë³‘í•©í•˜ê±°ë‚˜ ì„¤ì •ì— ë”°ë¼ ë¡œì»¬ì— ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---
